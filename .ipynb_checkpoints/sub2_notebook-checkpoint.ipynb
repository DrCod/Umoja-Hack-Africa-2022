{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RFh6Z7v5UQKi",
   "metadata": {
    "id": "RFh6Z7v5UQKi"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241c671-48c9-4034-b113-4c285e33b51b",
   "metadata": {
    "id": "5241c671-48c9-4034-b113-4c285e33b51b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T7FoKKB2zEA8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7FoKKB2zEA8",
    "outputId": "4e61ae16-9330-4386-acb3-119c76c6df83"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./folds/train_folds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162f6bb-0dfd-4310-a096-a584aee0f471",
   "metadata": {
    "id": "f162f6bb-0dfd-4310-a096-a584aee0f471"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"https://storage.googleapis.com/umojahack2022/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280d7c6-57df-4822-af33-e24ca6b13f2e",
   "metadata": {
    "id": "5280d7c6-57df-4822-af33-e24ca6b13f2e"
   },
   "outputs": [],
   "source": [
    "def get_seq_column_map(train, test, col):\n",
    "    sequences = []\n",
    "    for seq in train[col]:\n",
    "        sequences.extend(list(seq))\n",
    "    for seq in test[col]:\n",
    "        sequences.extend(list(seq))\n",
    "    unique = np.unique(sequences)\n",
    "    return {k: v for k, v in zip(unique, range(len(unique)))}\n",
    "\n",
    "def get_column_map(train, test, col):\n",
    "    sequences = []\n",
    "    unique_values = pd.concat([train[col], test[col]]).unique().tolist()\n",
    "    return {k: v for k, v in zip(unique_values, range(len(unique_values)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jTTjPAPN4b5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTTjPAPN4b5c",
    "outputId": "56559f4b-5c79-47c1-ba3c-fdbf937d98aa"
   },
   "outputs": [],
   "source": [
    "amino_acid_map = get_seq_column_map(train_df, test_df, \"Toxin_Kmer\")\n",
    "print(\"unique amino acid map\",len(amino_acid_map))\n",
    "\n",
    "antivenom_map = get_column_map(train_df, test_df, \"Antivenom\")\n",
    "print(\"unique Antivenom map\", len(antivenom_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pL58D6zMLZ9i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL58D6zMLZ9i",
    "outputId": "af6700cd-98c6-4c5f-fe32-971d2588077e"
   },
   "outputs": [],
   "source": [
    "amino_acid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-yGlKpQoLboj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yGlKpQoLboj",
    "outputId": "c9517dcf-c41c-4698-d3ad-5e8baf302299"
   },
   "outputs": [],
   "source": [
    "antivenom_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YSDgPC4YNm1w",
   "metadata": {
    "id": "YSDgPC4YNm1w"
   },
   "source": [
    "We will split the data into a training and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bbc44-ab69-403f-99ed-87ee2b7352f4",
   "metadata": {
    "id": "845bbc44-ab69-403f-99ed-87ee2b7352f4"
   },
   "outputs": [],
   "source": [
    "USE_FOLD = 0\n",
    "\n",
    "train_split_df = train_df[train_df['fold'] != USE_FOLD].reset_index(drop=True)\n",
    "val_split_df = train_df[train_df['fold'] == USE_FOLD].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49jlxU1QN4s3",
   "metadata": {
    "id": "49jlxU1QN4s3"
   },
   "source": [
    "We look at the GPU provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810517e6-008a-485c-8f6d-74db2cfb1770",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "810517e6-008a-485c-8f6d-74db2cfb1770",
    "outputId": "2a2c9df7-f552-4124-879d-dcca61e9ff5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y4HuvmEUODzZ",
   "metadata": {
    "id": "y4HuvmEUODzZ"
   },
   "source": [
    "We convert our data into a torch `Dataset`.\n",
    "All datasets that represent a map from keys to data samples should subclass\n",
    "`Dataset`. All subclasses should overwrite `__getitem__`, supporting fetching a data sample for a given key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38ed84-d4a2-4045-99b6-3165e4b7ea76",
   "metadata": {
    "id": "5c38ed84-d4a2-4045-99b6-3165e4b7ea76"
   },
   "outputs": [],
   "source": [
    "class AntivenomChallengeDataSet(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        amino_acid_map,\n",
    "        antivenom_map,\n",
    "        data,\n",
    "        is_train,\n",
    "        label_name=None,\n",
    "      ):\n",
    "        self.amino_acid_map = amino_acid_map\n",
    "        self.antivenom_map = antivenom_map\n",
    "        self.data = data\n",
    "        self.is_train = is_train\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        kmer_seq = torch.as_tensor([self.amino_acid_map[e] for e in list(row[\"Toxin_Kmer\"])])\n",
    "        antivenom = torch.as_tensor(self.antivenom_map[row[\"Antivenom\"]])\n",
    "        position_start = torch.as_tensor(row[\"Kmer_Position_start\"])\n",
    "        position_end = torch.as_tensor(row[\"Kmer_Position_end\"])\n",
    "        \n",
    "        inputs = {\n",
    "            \"K_mer\": kmer_seq,\n",
    "            \"antivenom\": antivenom,\n",
    "            \"position_start\": position_start,\n",
    "            \"position_end\": position_end,\n",
    "        }\n",
    "\n",
    "        if self.is_train: \n",
    "            return inputs, torch.as_tensor([row[self.label_name]])\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee359a-ab9f-422e-ae71-50685acd0c44",
   "metadata": {
    "id": "04ee359a-ab9f-422e-ae71-50685acd0c44"
   },
   "outputs": [],
   "source": [
    "train_dataset = AntivenomChallengeDataSet(\n",
    "    amino_acid_map=amino_acid_map,\n",
    "    antivenom_map=antivenom_map,\n",
    "    data=train_split_df,\n",
    "    is_train=True,\n",
    "    label_name=\"Signal\",\n",
    ")\n",
    "\n",
    "val_dataset = AntivenomChallengeDataSet(\n",
    "    amino_acid_map=amino_acid_map,\n",
    "    antivenom_map=antivenom_map,\n",
    "    data=val_split_df,\n",
    "    is_train=True,\n",
    "    label_name=\"Signal\",\n",
    ")\n",
    "\n",
    "test_dataset = AntivenomChallengeDataSet(\n",
    "    amino_acid_map=amino_acid_map,\n",
    "    antivenom_map=antivenom_map,\n",
    "    data=test_df,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cphwa_KMO6Um",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cphwa_KMO6Um",
    "outputId": "d8bb14dd-bab2-4d35-c474-c559a00801c6"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc84cdb-dfc7-4910-a83f-2d1a52ddd5e4",
   "metadata": {
    "id": "3cc84cdb-dfc7-4910-a83f-2d1a52ddd5e4"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "drop_last = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sQ15jMHhO3zh",
   "metadata": {
    "id": "sQ15jMHhO3zh"
   },
   "source": [
    "Now we create our PyTorch data loaders. These combine a dataset and a sampler, and provide an iterable over the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3ab3d-540b-4a84-ac68-61376aefb299",
   "metadata": {
    "id": "c0e3ab3d-540b-4a84-ac68-61376aefb299"
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=drop_last,\n",
    "    sampler=None,\n",
    "    pin_memory =False,\n",
    "\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,  # we do not want to drop the last batch during evaluation\n",
    "    pin_memory =False,\n",
    "\n",
    ")\n",
    "\n",
    "test_data_loader= DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    "    pin_memory =False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178cdf5-9440-4c1c-a012-9701a6363d35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4178cdf5-9440-4c1c-a012-9701a6363d35",
    "outputId": "2f9795f7-5540-488e-8b2a-769cc04b3c34"
   },
   "outputs": [],
   "source": [
    "x, y = iter(train_data_loader).next()\n",
    "\n",
    "print(f\"K_mer shape: {x['K_mer'].shape}\")\n",
    "print(f\"antivenom shape: {x['antivenom'].shape}\")\n",
    "print(f\"target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lXf0OdpP_zh",
   "metadata": {
    "id": "4lXf0OdpP_zh"
   },
   "source": [
    "## Define the model\n",
    "For this example we will build an LSTM architeture. It is your task to come up with more performant architectures to improve the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828aecc-9bb9-498e-89dd-25fb43482829",
   "metadata": {
    "id": "d828aecc-9bb9-498e-89dd-25fb43482829"
   },
   "outputs": [],
   "source": [
    "class SeqModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        K_mer_emb_size,\n",
    "        K_mer_nunique,\n",
    "        antivenom_emb_size,\n",
    "        antivenom_unique,\n",
    "        max_Position_start,\n",
    "        Position_start_emb_size,\n",
    "    ): \n",
    "        super().__init__()\n",
    "        self.K_mer_emb_size = K_mer_emb_size        \n",
    "        self.K_mer_nunique = K_mer_nunique                \n",
    "        self.antivenom_emb_size = antivenom_emb_size  \n",
    "        self.antivenom_unique = antivenom_unique    \n",
    "        \n",
    "        self.Kmer_emb_layer = nn.Embedding(\n",
    "            num_embeddings=self.K_mer_nunique,\n",
    "            embedding_dim=self.K_mer_emb_size,\n",
    "        )\n",
    "        self.Antivenom_emb = nn.Embedding(\n",
    "            num_embeddings=self.antivenom_unique,\n",
    "            embedding_dim=self.antivenom_emb_size,\n",
    "        )\n",
    "    \n",
    "        self.Position_start_emb = nn.Embedding(\n",
    "            num_embeddings=max_Position_start,\n",
    "            embedding_dim=Position_start_emb_size,\n",
    "        )\n",
    "        self.Features = nn.Linear(\n",
    "            in_features=self.antivenom_emb_size + Position_start_emb_size,\n",
    "            out_features=128,\n",
    "        )\n",
    "        \n",
    "        self.Lstm_layer_1 = nn.LSTM(\n",
    "            input_size=self.K_mer_emb_size,\n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.Lstm_layer_2 = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.Linear_1 = nn.Linear(\n",
    "            in_features=self.Lstm_layer_2.hidden_size + self.Features.out_features,\n",
    "            out_features=512,\n",
    "        )\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.Linear_2 = nn.Linear(\n",
    "            in_features=self.Linear_1.out_features, out_features=256,\n",
    "        )\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.Output = nn.Linear(\n",
    "            in_features=self.Linear_2.out_features, out_features=1,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        kmer_emb = self.Kmer_emb_layer(inputs[\"K_mer\"])\n",
    "        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n",
    "        position_start_emb = self.Position_start_emb(inputs[\"position_start\"])\n",
    "\n",
    "        emb_features = torch.cat((antivenom_emb, position_start_emb), axis=1)\n",
    "        features = self.Features(emb_features)\n",
    "        \n",
    "        lstm_1_seq, (lstm_1_h, lstm1_c) = self.Lstm_layer_1(kmer_emb)\n",
    "        lstm_2_seq, (lstm_2_h, lstm2_c) = self.Lstm_layer_2(lstm_1_seq)\n",
    "\n",
    "        lstm_h = torch.squeeze(lstm_2_h)\n",
    "        emb = torch.cat((lstm_h, features), axis=1)\n",
    "        emb = self.dropout(emb)\n",
    "        linear_1 = self.relu_1(self.Linear_1(emb))\n",
    "        linear_2 = self.relu_2(self.Linear_2(linear_1))\n",
    "        output = self.Output(linear_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Nu4DXk-9jyf",
   "metadata": {
    "id": "5Nu4DXk-9jyf"
   },
   "source": [
    "Now that the model architecture is defined we are goint to instantiate our model. For this we need to calculate `max_Position_start` in order to calculate the size of the embedding layer we will use to encode the start position. The maximum position that the train and test dataset can have is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pakRsAlg837A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pakRsAlg837A",
    "outputId": "65269828-2d9c-4689-8e32-26f66f36232a"
   },
   "outputs": [],
   "source": [
    "max_Position_start = pd.concat([train_df[[\"Kmer_Position_start\"]], test_df[[\"Kmer_Position_start\"]]]).Kmer_Position_start.max()+1\n",
    "\n",
    "print(f\"Max Position_start : {max_Position_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d277f2-aa03-4527-a6a7-044a8c90fe66",
   "metadata": {
    "id": "82d277f2-aa03-4527-a6a7-044a8c90fe66"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "early_stopping = 10\n",
    "\n",
    "model = SeqModel(\n",
    "    K_mer_emb_size=512,\n",
    "    K_mer_nunique=len(amino_acid_map),\n",
    "    antivenom_emb_size=64,\n",
    "    antivenom_unique=len(antivenom_map),\n",
    "    max_Position_start=max_Position_start,\n",
    "    Position_start_emb_size=64,\n",
    ")\n",
    "\n",
    "loss_fn = nn.HuberLoss(reduction='mean')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(model, {k: v.to(device) for k, v in next(iter(train_data_loader))[0].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wy_0FRUbJt_W",
   "metadata": {
    "id": "Wy_0FRUbJt_W"
   },
   "source": [
    "### Training the model\n",
    "We define a simple training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808e6b1-52bd-4685-a31a-fa665494613d",
   "metadata": {
    "id": "b808e6b1-52bd-4685-a31a-fa665494613d"
   },
   "outputs": [],
   "source": [
    "def train_func(\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    early_stopping=5,\n",
    "): \n",
    "    total_batches = len(train_data_loader)\n",
    "    total_batches_val = len(val_data_loader)\n",
    "    train_loss = []\n",
    "    \n",
    "    n_iter = 0\n",
    "    for epoch in range(num_epochs): \n",
    "        tqdm_bar = tqdm(train_data_loader, desc=f\"epoch {epoch}\", position=0) \n",
    "        old_val_loss = np.inf\n",
    "        wating = 0\n",
    "        model.train()\n",
    "        for batch_number, (X, y) in enumerate(tqdm_bar):\n",
    "            y = y.type(torch.FloatTensor).to(device)\n",
    "            X = {k: X[k].to(device) for k in X}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            writer.add_scalar(\"loss/train\", loss, n_iter)\n",
    "            n_iter += 1\n",
    "\n",
    "            if batch_number % 25 == 0: \n",
    "                tqdm_bar.set_postfix(\n",
    "                    {\n",
    "                        \"train\": f\"{batch_number}/{total_batches} loss: {loss:.3} epoch loss: {np.mean(train_loss):.3}\",\n",
    "                    },\n",
    "                )\n",
    "\n",
    "        val_tqdm_bar = tqdm(\n",
    "            val_data_loader, desc=f\"epoch {epoch}\", position=0, leave=True,\n",
    "        ) \n",
    "        val_loss = []\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for batch_number, (X, y) in enumerate(val_tqdm_bar):\n",
    "                y = y.type(torch.FloatTensor).to(device)\n",
    "                X = {k: X[k].to(device) for k in X}\n",
    "                \n",
    "                pred = model(X)\n",
    "                val_loss.append(loss_fn(pred, y).item())\n",
    "\n",
    "                writer.add_scalar(\"loss/validation\", np.random.random(), n_iter)\n",
    "\n",
    "                if batch_number % 25 == 0: \n",
    "                    val_tqdm_bar.set_postfix(\n",
    "                        {\n",
    "                            \"valid\": f\"{batch_number}/{total_batches_val} val loss: {np.mean(val_loss):.3}\"\n",
    "                        },\n",
    "                    )\n",
    "        \n",
    "        new_val_loss = np.mean(val_loss)\n",
    "\n",
    "        if new_val_loss > old_val_loss:\n",
    "            wating += wating\n",
    "        else:\n",
    "            old_val_loss = new_val_loss\n",
    "\n",
    "        if wating > early_stopping:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18984ce6-5cc7-403a-a223-1fadb7466706",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18984ce6-5cc7-403a-a223-1fadb7466706",
    "outputId": "aef667ac-2d54-492a-9310-388f02441b84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_func(\n",
    "    train_data_loader=train_data_loader,\n",
    "    val_data_loader=val_data_loader,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    early_stopping=early_stopping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d635a3-fa34-4b16-9a76-42dfb736455c",
   "metadata": {
    "id": "e8d635a3-fa34-4b16-9a76-42dfb736455c"
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"model2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07JhrgWQJjl3",
   "metadata": {
    "id": "07JhrgWQJjl3"
   },
   "source": [
    "### Sample baseline Submission\n",
    "Finally we will prepare a baseline submission to Zindi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NzqYHYOXGh1c",
   "metadata": {
    "id": "NzqYHYOXGh1c"
   },
   "outputs": [],
   "source": [
    "def predict_test(data_loader, path): \n",
    "    model = torch.load(path).to(device)\n",
    "    tqdm_bar = tqdm(data_loader, desc=\"Inference\", position=0, leave=True) \n",
    "    total_batches = len(tqdm_bar)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_number, X in enumerate(tqdm_bar):\n",
    "            X= {k: X[k].to(device) for k in X}\n",
    "            pred = model(X)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l25s3xDrJc8U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l25s3xDrJc8U",
    "outputId": "edf09615-089e-4a49-d18c-4bc312dcace3"
   },
   "outputs": [],
   "source": [
    "test_pred = predict_test(test_data_loader,\"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vWXzy2hZGiZq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWXzy2hZGiZq",
    "outputId": "63658f30-12cb-43ff-f230-b34f085d59aa"
   },
   "outputs": [],
   "source": [
    "sample_submission=test_df[[\"ID\"]]\n",
    "sample_submission[\"Signal\"] = test_pred.reshape((-1))\n",
    "sample_submission.to_csv(\"./submissions/sub2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BrBsX3OnGRVs",
   "metadata": {
    "id": "BrBsX3OnGRVs"
   },
   "source": [
    "That is it! Now we can upload the sample_submission.csv to Zindi! As a final thing lets look at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OVLFyzQK68uh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OVLFyzQK68uh",
    "outputId": "d277bea0-9b9f-4aad-c65d-1b8600689122"
   },
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eEOKd0Gm50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "34eEOKd0Gm50",
    "outputId": "00f64ca3-df40-4428-9016-f11dc21a78e1"
   },
   "outputs": [],
   "source": [
    "sample_submission[\"Signal\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WluOrYzKItCx",
   "metadata": {
    "id": "WluOrYzKItCx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Instadeep_StarterNotebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
